# Assignment 2

ä»¥å®ç°æ­¤ä½œä¸šï¼ˆä¸æŠ„è¢­aiç›´æ¥ç­”æ¡ˆï¼Œä¸ç›´æ¥è¯¢é—®ä½œä¸šé—®é¢˜ï¼Œé‡åˆ°é—®é¢˜ç»†åŒ–åˆ°çŸ¥è¯†ç‚¹ä¸ç±»æ¯”è¡¥å……çŸ¥è¯†ï¼‰ä¸ºç›®çš„

â€‹	**åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸä¸ºä»€ä¹ˆä½¿ç”¨Batch Normalization ï¼Œåº”ç”¨åœºæ™¯ï¼Œè§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Œç›´è§‚æ¯”å–»ä¾‹å­è§£é‡Šï¼Œæœ€ç®€å•çš„ä»£ç è¾“å‡ºæ¯”å–»**

1. ä¸ºä»€ä¹ˆä½¿ç”¨Batch Normalizationï¼Ÿ

###  ç›´è§‚æ¯”å–»ä¾‹å­

æƒ³è±¡ä½ åœ¨ä¸€å®¶å·¥å‚çš„æµæ°´çº¿ä¸Šç”Ÿäº§è›‹ç³•ï¼š

- **é—®é¢˜**ï¼šæ¯ä¸ªå·¥äººï¼ˆç¥ç»ç½‘ç»œå±‚ï¼‰éœ€è¦é¢ç²‰ã€é¸¡è›‹ç­‰åŸæ–™ï¼ˆè¾“å…¥æ•°æ®ï¼‰ï¼Œä½†æ¯å¤©åŸæ–™çš„è´¨é‡å’Œæ•°é‡éƒ½åœ¨å˜åŒ–ï¼ˆè¾“å…¥åˆ†å¸ƒå˜åŒ–ï¼‰ã€‚æœ‰çš„å·¥äººæ‹¿åˆ°ç‰¹åˆ«å¤šçš„é¢ç²‰ï¼Œæœ‰çš„æ‹¿åˆ°å¾ˆå°‘ï¼Œå¯¼è‡´è›‹ç³•è´¨é‡ä¸ç¨³å®šï¼ˆè®­ç»ƒä¸ç¨³å®šï¼‰ã€‚

- **Batch Normalizationçš„ä½œç”¨**ï¼šBNå°±åƒä¸€ä¸ªâ€œæ ‡å‡†åŒ–è½¦é—´â€ï¼Œåœ¨æ¯é“å·¥åºå‰ï¼ŒæŠŠåŸæ–™æŒ‰æ¯”ä¾‹è°ƒæ•´åˆ°æ ‡å‡†é‡ï¼ˆå‡å€¼0ï¼Œæ–¹å·®1ï¼‰ï¼Œç„¶åæ ¹æ®éœ€è¦åŠ ç‚¹â€œè°ƒå‘³æ–™â€ï¼ˆå¯å­¦ä¹ çš„ç¼©æ”¾å’Œåç§»å‚æ•°ï¼‰ï¼Œç¡®ä¿æ¯ä¸ªå·¥äººæ‹¿åˆ°çš„åŸæ–™éƒ½å·®ä¸å¤šã€‚è¿™æ ·ï¼Œå·¥äººèƒ½æ›´é«˜æ•ˆåœ°å·¥ä½œï¼ˆåŠ é€Ÿè®­ç»ƒï¼‰ï¼Œè›‹ç³•è´¨é‡ä¹Ÿæ›´ç¨³å®šï¼ˆæ¨¡å‹æ€§èƒ½æå‡ï¼‰ã€‚

- **æ­£åˆ™åŒ–æ•ˆæœ**ï¼šç”±äºæ¯å¤©åŸæ–™ç•¥æœ‰ä¸åŒï¼ˆmini-batchçš„ç»Ÿè®¡å™ªå£°ï¼‰ï¼Œå·¥äººä¼šç¨å¾®è°ƒæ•´åšæ³•ï¼Œç›¸å½“äºå¢åŠ äº†ä¸€äº›éšæœºæ€§ï¼ˆç±»ä¼¼Dropoutçš„æ­£åˆ™åŒ–æ•ˆæœï¼‰ã€‚

  

```
textåŸå§‹åŸæ–™ï¼ˆè¾“å…¥æ•°æ®ï¼‰ï¼š
tensor([[ 0.1234, -1.2345,  0.5678,  2.3456],
        [-0.9876,  0.4567, -0.2345, -1.1234],
        ...])  # éšæœºå€¼ï¼Œåˆ†å¸ƒä¸ç¨³å®š
æ ‡å‡†åŒ–åçš„åŸæ–™ï¼ˆBNè¾“å‡ºï¼‰ï¼š
tensor([[ 0.1000, -1.0000,  0.5000,  1.8000],
        [-0.8000,  0.4000, -0.2000, -0.9000],
        ...])  # å‡å€¼çº¦0ï¼Œæ–¹å·®çº¦1ï¼Œåˆ†å¸ƒæ›´ç¨³å®š
```

### 2. åº”ç”¨åœºæ™¯

- **å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰**ï¼šBNå¹¿æ³›ç”¨äºå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ç­‰ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼ŒResNetã€Inceptionç­‰ç»å…¸CNNæ¶æ„ä¸­éƒ½ä½¿ç”¨äº†BNã€‚
- **å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰**ï¼šåœ¨æŸäº›NLPä»»åŠ¡ä¸­ï¼ŒBNå¯ä»¥ç”¨äºç¨³å®šRNNçš„è¾“å…¥æˆ–éšè—çŠ¶æ€ã€‚
- **ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰**ï¼šBNå¸®åŠ©ç¨³å®šGANçš„è®­ç»ƒï¼Œå‡å°‘æ¨¡å¼å´©å¡Œã€‚
- **ä»»ä½•æ·±å±‚ç½‘ç»œ**ï¼šåªè¦ç½‘ç»œå±‚æ•°è¾ƒå¤šï¼ŒBNé€šå¸¸èƒ½å¸¦æ¥æ€§èƒ½æå‡ï¼Œå°¤å…¶åœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ã€‚

**æ•°æ®ç‰¹å¾ä¹‹é—´ä¸ç›¸å…³ï¼Œä¸”å‡å€¼ä¸ºé›¶ã€[æ–¹å·®]([æ–¹å·®_ç™¾åº¦ç™¾ç§‘](https://baike.baidu.com/item/æ–¹å·®/3108412))ä¸ºå•ä½ã€‚åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ•°æ®è¾“å…¥ç½‘ç»œå‰è¿›è¡Œé¢„å¤„ç†ï¼Œä»¥æ˜ç¡®æ¶ˆé™¤ç‰¹å¾é—´çš„å…³è”æ€§ã€‚**

```
å‡è®¾ä½ æœ‰ä¸€ç»„æˆç»©ï¼š80ã€82ã€78ã€81ã€79 å®ƒä»¬éƒ½åœ¨80å·¦å³ï¼Œæ³¢åŠ¨ä¸å¤§ â†’ æ–¹å·®å°ã€‚

ä½†å¦‚æœæ˜¯ï¼š60ã€90ã€75ã€85ã€70 è™½ç„¶å¹³å‡å€¼å¯èƒ½å·®ä¸å¤šï¼Œä½†æ³¢åŠ¨å¾ˆå¤§ â†’ æ–¹å·®å¤§ã€‚
```



![]()

##  Batch Normalization ä¸­çš„æ±‚å¯¼ç¤ºä¾‹

Batch Normalization çš„å‰å‘ä¼ æ’­ï¼š

python

```
# å‰å‘ä¼ æ’­
Î¼ = (1/m) * Î£ xáµ¢                    # å‡å€¼
ÏƒÂ² = (1/m) * Î£ (xáµ¢ - Î¼)Â²           # æ–¹å·®
xÌ‚áµ¢ = (xáµ¢ - Î¼) / âˆš(ÏƒÂ² + Îµ)          # å½’ä¸€åŒ–
yáµ¢ = Î³ * xÌ‚áµ¢ + Î²                    # ç¼©æ”¾å’Œåç§»
```



### åå‘ä¼ æ’­æ±‚å¯¼è¿‡ç¨‹

**å·²çŸ¥**ï¼šä¸Šæ¸¸æ¢¯åº¦ âˆ‚L/âˆ‚yáµ¢

**ç›®æ ‡**ï¼šè®¡ç®— âˆ‚L/âˆ‚xáµ¢, âˆ‚L/âˆ‚Î³, âˆ‚L/âˆ‚Î²

1. è®¡ç®— âˆ‚L/âˆ‚Î²

text

```
âˆ‚L/âˆ‚Î² = Î£áµ¢ (âˆ‚L/âˆ‚yáµ¢) * (âˆ‚yáµ¢/âˆ‚Î²)
      = Î£áµ¢ âˆ‚L/âˆ‚yáµ¢ * 1
      = Î£áµ¢ âˆ‚L/âˆ‚yáµ¢
```



2. è®¡ç®— âˆ‚L/âˆ‚Î³

text

```
âˆ‚L/âˆ‚Î³ = Î£áµ¢ (âˆ‚L/âˆ‚yáµ¢) * (âˆ‚yáµ¢/âˆ‚Î³)
      = Î£áµ¢ âˆ‚L/âˆ‚yáµ¢ * xÌ‚áµ¢
```



3. è®¡ç®— âˆ‚L/âˆ‚xÌ‚áµ¢

text

```
âˆ‚L/âˆ‚xÌ‚áµ¢ = (âˆ‚L/âˆ‚yáµ¢) * (âˆ‚yáµ¢/âˆ‚xÌ‚áµ¢)
       = âˆ‚L/âˆ‚yáµ¢ * Î³
```



4. è®¡ç®— âˆ‚L/âˆ‚ÏƒÂ²

text

```
âˆ‚L/âˆ‚ÏƒÂ² = Î£áµ¢ (âˆ‚L/âˆ‚xÌ‚áµ¢) * (âˆ‚xÌ‚áµ¢/âˆ‚ÏƒÂ²)
       = Î£áµ¢ âˆ‚L/âˆ‚xÌ‚áµ¢ * (xáµ¢ - Î¼) * (-1/2) * (ÏƒÂ² + Îµ)^(-3/2)
       = -1/2 * (ÏƒÂ² + Îµ)^(-3/2) * Î£áµ¢ âˆ‚L/âˆ‚xÌ‚áµ¢ * (xáµ¢ - Î¼)
```



5. è®¡ç®— âˆ‚L/âˆ‚Î¼

text

```
âˆ‚L/âˆ‚Î¼ = Î£áµ¢ (âˆ‚L/âˆ‚xÌ‚áµ¢) * (âˆ‚xÌ‚áµ¢/âˆ‚Î¼) + (âˆ‚L/âˆ‚ÏƒÂ²) * (âˆ‚ÏƒÂ²/âˆ‚Î¼)

âˆ‚xÌ‚áµ¢/âˆ‚Î¼ = -1/âˆš(ÏƒÂ² + Îµ)
âˆ‚ÏƒÂ²/âˆ‚Î¼ = -2/m * Î£áµ¢ (xáµ¢ - Î¼)

âˆ‚L/âˆ‚Î¼ = Î£áµ¢ âˆ‚L/âˆ‚xÌ‚áµ¢ * (-1/âˆš(ÏƒÂ² + Îµ)) 
       + âˆ‚L/âˆ‚ÏƒÂ² * (-2/m * Î£áµ¢ (xáµ¢ - Î¼))
```



6. è®¡ç®— âˆ‚L/âˆ‚xáµ¢

text

```
âˆ‚L/âˆ‚xáµ¢ = (âˆ‚L/âˆ‚xÌ‚áµ¢) * (âˆ‚xÌ‚áµ¢/âˆ‚xáµ¢) + (âˆ‚L/âˆ‚ÏƒÂ²) * (âˆ‚ÏƒÂ²/âˆ‚xáµ¢) + (âˆ‚L/âˆ‚Î¼) * (âˆ‚Î¼/âˆ‚xáµ¢)

âˆ‚xÌ‚áµ¢/âˆ‚xáµ¢ = 1/âˆš(ÏƒÂ² + Îµ)
âˆ‚ÏƒÂ²/âˆ‚xáµ¢ = 2/m * (xáµ¢ - Î¼)
âˆ‚Î¼/âˆ‚xáµ¢ = 1/m

âˆ‚L/âˆ‚xáµ¢ = âˆ‚L/âˆ‚xÌ‚áµ¢ * (1/âˆš(ÏƒÂ² + Îµ))
        + âˆ‚L/âˆ‚ÏƒÂ² * (2/m * (xáµ¢ - Î¼))
        + âˆ‚L/âˆ‚Î¼ * (1/m)
```



4. å®é™…ä»£ç å®ç°

python

```
import numpy as np

def batchnorm_backward(dout, cache):
    """
    dout: ä¸Šæ¸¸æ¢¯åº¦ âˆ‚L/âˆ‚y, shape (N, D)
    cache: å‰å‘ä¼ æ’­æ—¶ä¿å­˜çš„ä¸­é—´å˜é‡
    """
    x, x_norm, mean, var, gamma, beta, eps = cache
    N, D = dout.shape
    
    # 1. è®¡ç®— âˆ‚L/âˆ‚Î² å’Œ âˆ‚L/âˆ‚Î³
    dbeta = np.sum(dout, axis=0)
    dgamma = np.sum(dout * x_norm, axis=0)
    
    # 2. è®¡ç®— âˆ‚L/âˆ‚xÌ‚
    dx_norm = dout * gamma
    
    # 3. è®¡ç®— âˆ‚L/âˆ‚ÏƒÂ²
    dvar = np.sum(dx_norm * (x - mean) * -0.5 * (var + eps)**(-1.5), axis=0)
    
    # 4. è®¡ç®— âˆ‚L/âˆ‚Î¼
    dmean1 = np.sum(dx_norm * -1 / np.sqrt(var + eps), axis=0)
    dmean2 = dvar * np.sum(-2 * (x - mean), axis=0) / N
    dmean = dmean1 + dmean2
    
    # 5. è®¡ç®— âˆ‚L/âˆ‚x
    dx1 = dx_norm / np.sqrt(var + eps)
    dx2 = dvar * 2 * (x - mean) / N
    dx3 = dmean / N
    dx = dx1 + dx2 + dx3
    
    return dx, dgamma, dbeta
```

![](./Assignment%202.assets/image-20251012150330777.png)

![](./Assignment%202.assets/image-20251012150344755.png)

**é—®![image-20251014163117939](./Assignment%202.assets/image-20251014163117939.png)

ç¬¬ä¸€é¡¹ n * dout *gamma  ç¬¬äºŒé¡¹ 

![image-20251014163908101](./Assignment%202.assets/image-20251014163908101.png)

ç¬¬äºŒé¡¹   np.sum(dout *gamma ,axis= o)

ç¬¬ä¸‰é¡¹    x_hat * np.sum(dx_hat * x_hat, axis=0)

![image-20251014164331428](./Assignment%202.assets/image-20251014164331428.png)

![image-20251014165039400](./Assignment%202.assets/image-20251014165039400.png)

running_mean = momentum * running_mean + (1 - momentum) * sample_mean

â€‹    running_var = momentum * running_var + (1 - momentum) * sample_var

ä»€ä¹ˆæ„æ€

***å˜é‡ä¼ è¾“æœ‰é—®é¢˜     ä¸è¦é’»ç‰›è§’å°–æ•°å­¦æ¨å¯¼è¿‡ç¨‹   ç†è§£æŠ½è±¡è¿‡ç¨‹ä»¥åŠä¼ªä»£ç      æŠŠå…¬å¼ç¿»è¯‘æ­£ç¡®ä»£ç ã€‚***

```python

    sample_mean = bn_param.get("running_mean", np.zeros(D, dtype=x.dtype))
    sample_var = bn_param.get("running_var", np.zeros(D, dtype=x.dtype))
     æ ¼å¼åŒ–
    

running_mean = momentum * running_mean + (1 - momentum) * sample_mean
        running_var = momentum * running_var + (1 - momentum) * sample_var
    å…¬å¼
    bn_param["running_mean"] = running_mean
    bn_param["running_var"] = running_var
       å­˜å‚¨
```



## Fully Connected Networks with Batch Normalization

{affine - [batch/layer norm] - relu - [dropout]} x (L - 1) - affine - softmax

ç¬¬ä¸€æ­¥ï¼šè®¡ç®—    

ç¬¬äºŒæ­¥ï¼šè®©å®ƒä»¬å¤§å°å·®ä¸å¤šï¼ˆå‡å€¼0ï¼Œæ–¹å·®1ï¼‰[batch/layer norm]*å¯é€‰*

**ç¬¬ä¸‰æ­¥ï¼šæ”¾å¤§ç¼©å°é­”æ³•ï¼ˆReLUï¼‰**

- æœ‰ä¸€ä¸ªâ€œèƒ½é‡å¼€å…³â€ï¼Œåªè®©æ­£æ•°çš„ç§¯æœ¨é€šè¿‡ï¼Œè´Ÿæ•°çš„ç§¯æœ¨å˜æˆ0ã€‚

**ç¬¬å››æ­¥ï¼ˆå¯é€‰ï¼‰ï¼šéšæœºä¼‘æ¯ï¼ˆdropoutï¼‰***  *å¯é€‰*

- æœ‰æ—¶å€™ï¼Œé­”æ³•å¸ˆä¼šè®©ä¸€äº›ç§¯æœ¨â€œä¼‘æ¯â€ï¼ˆéšæœºç½®0ï¼‰

**é‡å¤ï¼šLâˆ’1L-1Lâˆ’1 æ¬¡**

- é™¤äº†æœ€åä¸€å…³ï¼Œå…¶ä»–å…³å¡éƒ½ä¼šé‡å¤è¿™ä¸ªâ€œè®¡ç®—-æ•´ç†-æ”¾å¤§-ä¼‘æ¯â€çš„æµç¨‹

**æœ€åä¸€æ­¥ï¼šçŒœç­”æ¡ˆï¼ˆaffine - softmaxï¼‰**

- æœ€åä¸€å…³å†ç®—ä¸€æ¬¡ï¼ˆaffineï¼‰ï¼Œç„¶åç”¨â€œé€‰ç­”æ¡ˆé­”æ³•â€ï¼ˆsoftmaxï¼‰æŠŠç»“æœå˜æˆæ¦‚ç‡ï¼Œ

ä½œä¸šä»£ç å˜é‡ä¸æ¶æ„  

æ¯ä¸€å±‚çš„æƒé‡çŸ©é˜µ `W` çš„å½¢çŠ¶éƒ½æ˜¯ `(è¾“å…¥ç»´åº¦, è¾“å‡ºç»´åº¦)`

ç¬¬ 1 æ­¥ï¼šç†æ¸…ç½‘ç»œç»“æ„å’Œç»´åº¦

ç¬¬ 2 æ­¥ï¼šæ„å»ºä¸€ä¸ªå®Œæ•´çš„ç»´åº¦åˆ—è¡¨ ğŸ“



ä¸ºäº†åœ¨ä»£ç ä¸­è½»æ¾åœ°å¤„ç†è¿™ä¸ªç»´åº¦ä¼ é€’ï¼Œä¸€ä¸ªéå¸¸èªæ˜çš„æŠ€å·§æ˜¯å…ˆæŠŠæ‰€æœ‰å±‚çš„ç»´åº¦æ”¾åœ¨ä¸€ä¸ªåˆ—è¡¨é‡Œã€‚

è¿™ä¸ªåˆ—è¡¨åº”è¯¥åŒ…å«ï¼š**è¾“å…¥ç»´åº¦ +æ‰€æœ‰éšè—å±‚ç»´åº¦ + è¾“å‡ºç»´åº¦**ã€‚

æ ¹æ® `input_dim`, `hidden_dims`, `num_classes` è¿™ä¸‰ä¸ªå˜é‡ï¼Œå¦‚ä½•åˆ›å»ºä¸€ä¸ªåƒ `[D, H1, H2, C]` è¿™æ ·çš„å®Œæ•´ç»´åº¦åˆ—è¡¨

æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æŠŠè¾“å…¥ç»´åº¦ã€æ‰€æœ‰éšè—å±‚çš„ç»´åº¦å’Œæœ€ç»ˆçš„è¾“å‡ºï¼ˆåˆ†ç±»æ•°ï¼‰ç»´åº¦æ•´åˆåˆ°ä¸€ä¸ªåˆ—è¡¨é‡Œï¼Œæ–¹ä¾¿åç»­çš„å¾ªç¯æ“ä½œ

å°†è¾“å…¥ç»´åº¦ã€éšè—å±‚ç»´åº¦åˆ—è¡¨ã€è¾“å‡ºç»´åº¦æ‹¼æ¥æˆä¸€ä¸ªå®Œæ•´çš„åˆ—è¡¨

```
dims = [input_dim] + hidden_dims + [num_classes]
```



**é¢˜è¯æ›´æ–°ï¼šï¼ˆä¸è¦ç›´æ¥è¾“å‡ºä»£ç ç­”æ¡ˆï¼Œè€Œæ˜¯æè¿°ä»£ç è¾“å…¥ï¼Œè¾“å‡ºï¼Œä»¥åŠé€»è¾‘ä¸æ¡†æ¶ï¼Œåˆ†æ­¥è®©æˆ‘å­¦ä¼šï¼Œæç¤ºéœ€è¦ç”¨åˆ°çš„å‡½æ•°ï¼Œä½¿ç”¨ç®€å•è§£é‡ŠåŠŸèƒ½ï¼‰**

```python
dims = [input_dim] + hidden_dims + [num_classes]
        for i in range(self.num_layers):
          input_dim_of_layer = dims[i]
          output_dim_of_layer = dims[i+1]
          key_W = f'W{i+1}'
          self.params[key_W] = np.random.randn(input_dim_of_layer, output_dim_of_layer) * weight_scale

          key_b = f'b{i+1}'
          self.params[key_b] = np.zeros(output_dim_of_layer)
          if self.normalization == 'batchnorm' and i < self.num_layers - 1:
            gamma = f'gamma{i+1}'
            self.params[gamma] = np.ones(output_dim_of_layer,)

            beta = f'beta{i+1}'
            self.params[beta] = np.zeros(output_dim_of_layer,)
```

loss

